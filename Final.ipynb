{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Popular Description Words using Goodreads Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n",
    "\n",
    "Topics that are covered in this tutorial are:\n",
    "- Loading data from Goodreads API\n",
    "- Processing descriptions into meaningful words\n",
    "- Most common words used in general\n",
    "- Most common words used, grouped by popularity of the book\n",
    "- Most common words used, grouped by rating of the book\n",
    "- Most common words used overtime\n",
    "- Most common words used in reviews, grouped by rating of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, json\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from Goodreads API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yfmkhh2aWg8dnQyvnoRXg\n"
     ]
    }
   ],
   "source": [
    "def read_api_key(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return f.read().replace('\\n','')\n",
    "    \n",
    "api_key = read_api_key('api_key.txt')\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test function for querying goodreads API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def goodreads_query(api_key):\n",
    "    \"\"\"\n",
    "    api_key (string): API key\n",
    "    url (string): Base query URL\n",
    "    query (dictionary): Query parameters\n",
    "    \"\"\"\n",
    "    request_params = {'id':68428, 'key':api_key, 'format':'xml', 'text_only':'true'}\n",
    "    response = requests.get('https://www.goodreads.com/book/show', params=request_params)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    shelves = soup.find('popular_shelves')\n",
    "    return response\n",
    "\n",
    "temp_response = goodreads_query(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get metadata about a book given its book ID. \n",
    "\n",
    "Metadata includes: book_id, author, publication_date(month/day/year), publisher, review_count, rating_count, average_rating and genre and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get metadata about a book given its book ID\n",
    "def get_book_metadata(api_key, book_id):\n",
    "    request_params = {'id':book_id, 'key':api_key, 'format':'xml', 'text_only':'true'}\n",
    "    response = requests.get('https://www.goodreads.com/book/show', params=request_params)\n",
    "    \n",
    "    book_info = {}\n",
    "    book_info['book_id'] = book_id\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Author\n",
    "    book_info['author'] = soup.find('author').find('name').text\n",
    "    \n",
    "    # Publication date\n",
    "    year = soup.find('publication_year').text\n",
    "    month = soup.find('publication_month').text\n",
    "    day = soup.find('publication_day').text\n",
    "    book_info['publication_date'] = str(month) + '/' + str(day) + '/' + str(year)\n",
    "    \n",
    "    # Publisher\n",
    "    book_info['publisher'] = soup.find('publisher').text\n",
    "    \n",
    "    # Review information\n",
    "    book_info['review_count'] = soup.find('text_reviews_count').text\n",
    "    \n",
    "    # Rating information\n",
    "    book_info['rating_count'] = soup.find('ratings_count').text\n",
    "    book_info['average_rating'] = soup.find('average_rating').text\n",
    "    \n",
    "    # Treat the genre as the most popular shelf this book has been placed on\n",
    "    book_info['genre'] = soup.find('popular_shelves').find('shelf')['name']\n",
    "    \n",
    "    return book_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded 23 csv files of 10k books each, totaling in 230k books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save all book metadata as a CSV\n",
    "def download_book_metadata(api_key):\n",
    "    data_columns = ['book_id', 'author', 'publication_date', 'publisher', 'review_count', 'rating_count', 'average_rating', 'genre']\n",
    "    \n",
    "    with open('metadata.csv', 'w', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, data_columns, lineterminator='\\n')\n",
    "        dict_writer.writeheader()\n",
    "        \n",
    "        # TODO: iterate through book IDs - add a time.sleep(1) between each iteration\n",
    "        current_metadata = get_book_metadata(api_key, 68428)\n",
    "        dict_writer.writerow(current_metadata)\n",
    "\n",
    "download_book_metadata(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the reviews for a given book ID as a list of dictionaries.\n",
    "\n",
    "Review data includes: book_id, rating, date (day-month-year), and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reviews(api_key, book_id):\n",
    "    request_params = {'id':book_id, 'key':api_key, 'format':'xml', 'text_only':'true'}\n",
    "    response = requests.get('https://www.goodreads.com/book/show', params=request_params)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    iframe = soup.find('reviews_widget').find('iframe')\n",
    "    \n",
    "    all_reviews = []\n",
    "    \n",
    "    reviews_url = iframe['src']\n",
    "    response_reviews = requests.get(reviews_url)\n",
    "    \n",
    "    while(1):\n",
    "        review_soup = BeautifulSoup(response_reviews.text, 'lxml')\n",
    "        reviews = review_soup.find_all('div', {'class':'gr_review_container'})\n",
    "\n",
    "        for review in reviews:\n",
    "            current_review = {}\n",
    "\n",
    "            # Extract the rating\n",
    "            rating = review.find('span', {'class':'gr_rating'})\n",
    "            if (rating):\n",
    "                rating = rating.find_all(text=True)\n",
    "                rating_num = 0\n",
    "                for i in range(len(rating[0])):\n",
    "                    if (ord(rating[0][i]) == 9733):\n",
    "                        rating_num += 1\n",
    "            else:\n",
    "                rating_num = -1\n",
    "\n",
    "            # Extract the date\n",
    "            date = review.find('span', {'class':'gr_review_date'}).find_all(text=True)\n",
    "            date = [x.strip() for x in date][0]\n",
    "\n",
    "            # Extract the review text\n",
    "            s = [x.strip() for x in review.find('div', {'class':'gr_review_text'}).find_all(text=True)]\n",
    "            s = [x for x in s if x]\n",
    "\n",
    "            # Ignore the last element, the '...more'\n",
    "            review_text = ' '.join(s[:-1])\n",
    "\n",
    "            # Remove the last word, since it will be partial\n",
    "            review_text = ' '.join(review_text.split(' ')[:-1])\n",
    "\n",
    "            current_review['book_id'] = book_id\n",
    "            current_review['rating'] = rating_num\n",
    "            current_review['date'] = date\n",
    "            current_review['text'] = review_text\n",
    "\n",
    "            all_reviews.append(current_review)\n",
    "\n",
    "        # See if there is another page of reviews\n",
    "        if (review_soup.find_all('a', {'class':'next_page'}) != []):\n",
    "            # There is another page\n",
    "            reviews_url = 'https://goodreads.com' + review_soup.find('a', {'class':'next_page'})['href']\n",
    "            response_reviews = requests.get(reviews_url)\n",
    "        else:\n",
    "            # No more pages of reviews\n",
    "            break\n",
    "        \n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded 5 csv files of 10k reviews each, totaling in 60k books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_reviews(api_key):\n",
    "    data_columns = ['book_id', 'rating', 'date', 'text']\n",
    "    \n",
    "    with open('reviews.csv', 'w', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, data_columns, lineterminator='\\n')\n",
    "        dict_writer.writeheader()\n",
    "        \n",
    "        # TODO: iterate through book IDs - add a time.sleep(1) between each iteration\n",
    "        current_reviews = get_reviews(api_key, 68428)\n",
    "        dict_writer.writerows(current_reviews)\n",
    "\n",
    "book_reviews = get_reviews(api_key, 68428)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing descriptions into meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_df(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[pd.notnull(df['description'])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take out the words book and author, since it occurs the most times in most categories, yet it seemes meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    for char in text: \n",
    "        if (char in string.punctuation): text = text.replace(char, \" \")\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word, 'v') for word in words]\n",
    "    filtered_words = [word for word in words if word not in stopwords \\\n",
    "                      and word != 'book' and word != 'autho' and word != \"'\" and word != '\"']\n",
    "    return filtered_words\n",
    "\n",
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    for i, row in df.iterrows():\n",
    "        original = row['description']\n",
    "        df.at[i, 'description'] = process(original)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load 23 files of metadata and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'metadata/metadata1.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-692f923f8e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metadata/metadata%d.csv'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d28298fcdc5f>\u001b[0m in \u001b[0;36mcsv_to_df\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcsv_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'metadata/metadata1.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df_full = []\n",
    "for x in range(1,24):\n",
    "    df = csv_to_df('metadata/metadata%d.csv'%(x))\n",
    "    processed = process_all(df)\n",
    "    df_full.append(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words used in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def popular_words(processed):\n",
    "    fin = []\n",
    "    for i, row in processed.iterrows():\n",
    "        fin.extend(row['description'])\n",
    "    count = Counter(fin)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter_all = Counter()\n",
    "for i in range(0,23):\n",
    "    processed = df_full[i]\n",
    "    rare = popular_words(processed)\n",
    "    counter_all += rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few significant words seems to be: life, world, time, love, and man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter_all.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words used, grouped by popularity of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that rating count is a good way to determine the popularity of the book. By visualizing the distribution of the rating count, we decide on the bin for popularity to be: 0 to 100k, 100k to 200k,  200k to 300k,  300k to 400k, 400k to 500k,  500k to 600k, 600k to 700k, 700k to 800k, 800k to 900k, 900k to 1000k, 1000k to 3000k and 3000kabove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,24):\n",
    "    df = csv_to_df('metadata/metadata%d.csv'%(x))\n",
    "    plt.hist(df['rating_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def popular_words_rc(processed):\n",
    "    R0_100k, R100k_200k,  R200k_300k,  R300k_400k, R400k_500k,  R500k_600k, R600k_700k, \\\n",
    "    R700k_800k, R800k_900k, R900k_1000k, R1000k_3000k, R3000kabove \\\n",
    "                = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "    for i, row in processed.iterrows():\n",
    "        if (0 <= row['rating_count'] < 100000): R0_100k.extend(row['description'])\n",
    "        elif (100000 <= row['rating_count'] < 200000): R100k_200k.extend(row['description'])\n",
    "        elif (200000 <= row['rating_count'] < 300000): R200k_300k.extend(row['description'])\n",
    "        elif (300000 <= row['rating_count'] < 400000): R300k_400k.extend(row['description'])\n",
    "        elif (400000 <= row['rating_count'] < 500000): R400k_500k.extend(row['description'])\n",
    "        elif (500000 <= row['rating_count'] < 600000): R500k_600k.extend(row['description'])\n",
    "        elif (600000 <= row['rating_count'] < 700000): R600k_700k.extend(row['description'])\n",
    "        elif (700000 <= row['rating_count'] < 800000): R700k_800k.extend(row['description'])\n",
    "        elif (800000 <= row['rating_count'] < 900000): R800k_900k.extend(row['description'])\n",
    "        elif (900000 <= row['rating_count'] < 1000000): R900k_1000k.extend(row['description'])\n",
    "        elif (1000000 <= row['rating_count'] < 3000000): R1000k_3000k.extend(row['description'])\n",
    "        elif (3000000 <= row['rating_count']): R3000kabove.extend(row['description'])\n",
    "            \n",
    "    return Counter(R0_100k), Counter(R100k_200k), Counter(R200k_300k), Counter(R300k_400k), \\\n",
    "            Counter(R400k_500k), Counter(R500k_600k), Counter(R600k_700k), Counter(R700k_800k), \\\n",
    "            Counter(R800k_900k), Counter(R900k_1000k), Counter(R1000k_3000k), Counter(R3000kabove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R0_100k, R100k_200k,  R200k_300k,  R300k_400k, R400k_500k,  R500k_600k, R600k_700k, \\\n",
    "    R700k_800k, R800k_900k, R900k_1000k, R1000k_3000k, R3000kabove \\\n",
    "        = Counter(), Counter(), Counter(), Counter(), Counter(), Counter(), Counter(), Counter(),\\\n",
    "            Counter(), Counter(), Counter(), Counter()\n",
    "for x in range(0,23):\n",
    "    processed = df_full[x]\n",
    "    a, b, c, d, e, f, g, h, i, j, k, l = popular_words_rc(processed)\n",
    "    R0_100k += a\n",
    "    R100k_200k += b\n",
    "    R200k_300k += c\n",
    "    R300k_400k += d\n",
    "    R400k_500k += e\n",
    "    R500k_600k += f\n",
    "    R600k_700k += g\n",
    "    R700k_800k += h\n",
    "    R800k_900k += i\n",
    "    R900k_1000k += j\n",
    "    R1000k_3000k += k\n",
    "    R3000kabove += l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that books that were extremely popular included words like harry (Harry Potter), gatsby (The Great Gatsby), and kill (To Kill a Mockingbird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"0 to 100k:   \" + str(R0_100k.most_common(10)) + \"\\n\")\n",
    "print(\"100k to 200k:   \" + str(R100k_200k.most_common(10))+ \"\\n\")\n",
    "print(\"200k to 300k:   \" + str(R200k_300k.most_common(10))+ \"\\n\")\n",
    "print(\"300k to 400k:   \" + str(R300k_400k.most_common(10))+ \"\\n\")\n",
    "print(\"400k to 500k:   \" + str(R400k_500k.most_common(10))+ \"\\n\")\n",
    "print(\"500k to 600k:   \" + str(R500k_600k.most_common(10))+ \"\\n\")\n",
    "print(\"600k to 700k:   \" + str(R600k_700k.most_common(10))+ \"\\n\")\n",
    "print(\"700k to 800k:   \" + str(R700k_800k.most_common(10))+ \"\\n\")\n",
    "print(\"800k to 900k:   \" + str(R800k_900k.most_common(10))+ \"\\n\")\n",
    "print(\"900k to 1000k:   \" + str(R900k_1000k.most_common(10))+ \"\\n\")\n",
    "print(\"1000k to 3000k:   \" + str(R1000k_3000k.most_common(10))+ \"\\n\")\n",
    "print(\"3000k above:   \" + str(R3000kabove.most_common(10))+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words used, grouped by rating of the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualizing the distribution of the rating count, we decide on the bin for popularity to be: 0 to 1, 1 to 2, 2 to 2.5, 2.5 to 3, 3 to 3.3, 3.3 to 3.6, 3.6 to 3.9, 3.9 to 4.2, 4.2 to 4.5 and 4.5 to 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,24):\n",
    "    df = csv_to_df('metadata/metadata%d.csv'%(x))\n",
    "    plt.hist(df['average_rating'], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def popular_words_rating(processed):\n",
    "    zero, one, two, two2, three1, three2, three3, four1, four2 = \\\n",
    "    [], [], [], [], [], [], [], [], []\n",
    "    for i, row in processed.iterrows():\n",
    "        if (0 <= round(row['average_rating'], 1) < 1.0): zero.extend(row['description'])\n",
    "        elif (1 <= round(row['average_rating'], 1) < 2.0): one.extend(row['description'])\n",
    "        elif (2.0 <= round(row['average_rating'], 1) < 2.5): two.extend(row['description'])\n",
    "        elif (2.5 <= round(row['average_rating'], 1) < 3.0): two2.extend(row['description'])\n",
    "        elif (3.0 <= round(row['average_rating'], 1) < 3.3): three1.extend(row['description'])\n",
    "        elif (3.3 <= round(row['average_rating'], 1) < 3.6): three2.extend(row['description'])\n",
    "        elif (3.6 <= round(row['average_rating'], 1) < 3.9): three3.extend(row['description'])\n",
    "        elif (4.2 <= round(row['average_rating'], 1) < 4.5): four1.extend(row['description'])\n",
    "        elif (4.5 <= round(row['average_rating'], 1) < 5.0): four2.extend(row['description'])\n",
    "    return Counter(zero), Counter(one), Counter(two), Counter(two2), Counter(three1), \\\n",
    "            Counter(three2), Counter(three3), Counter(four1), Counter(four2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero, one, two, two2, three1, three2, three3, four1, four2 = \\\n",
    "    Counter(), Counter(), Counter(), Counter(), Counter(), Counter(), Counter(), Counter(), Counter()\n",
    "for x in range(0,23):\n",
    "    processed = df_full[x]\n",
    "    a, b, c, d, e, f, g, h, i= popular_words_rating(processed)\n",
    "    zero += a\n",
    "    one += b\n",
    "    two += c\n",
    "    two2 += d\n",
    "    three1 += e\n",
    "    three2 += f\n",
    "    three3 += g\n",
    "    four1 += h\n",
    "    four2 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rating 0.0 to 1.0 :   \" + str(zero.most_common(20)) + \"\\n\")\n",
    "print(\"rating 1.0 to 2.0 :   \" + str(one.most_common(10)) + \"\\n\")\n",
    "print(\"rating 2.0 to 2.5 :   \" + str(two.most_common(10)) +\"\\n\")\n",
    "print(\"rating 2.5 to 3.0 :   \" + str(two2.most_common(20)) + \"\\n\")\n",
    "print(\"rating 3.0 to 3.3:   \" + str(three1.most_common(20)) + \"\\n\")\n",
    "print(\"rating 3.3 to 3.6 :   \" + str(three2.most_common(20)) + \"\\n\")\n",
    "print(\"rating 3.6 to 3.9:   \" + str(three3.most_common(20)) + \"\\n\")\n",
    "print(\"rating 4.2 to 4.5 :   \" + str(four1.most_common(20)) + \"\\n\")\n",
    "print(\"rating 4.5 to 5.0 :   \" + str(four2.most_common(20)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words used overtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load deta frames again and this time, get rid of all books with no publication data and ignore month and date of publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = []\n",
    "for x in range(1,24):\n",
    "    df = csv_to_df('metadata/metadata%d.csv'%(x))\n",
    "    df = df[df.publication_date != '//']\n",
    "    df[\"publication_date\"] = df[\"publication_date\"].apply(lambda x: x[-4:])\n",
    "    processed = process_all(df)\n",
    "    df_time.append(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def popular_words_time(processed):\n",
    "    t1900,t1950, t1960, t1970, t1980, t1990, t1995, t2000, t2002,\\\n",
    "    t2005,t2007, t2010, t2012, t2015, t2020  = [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "    \n",
    "    for i, row in processed.iterrows():\n",
    "        if (1800 < int(row['publication_date']) <= 1900): t1900.extend(row['description'])\n",
    "        elif (1900 < int(row['publication_date']) <= 1950): t1950.extend(row['description'])\n",
    "        elif (1950 < int(row['publication_date']) <= 1960): t1960.extend(row['description'])\n",
    "        elif (1960 < int(row['publication_date']) <= 1970): t1970.extend(row['description'])\n",
    "        elif (1970 < int(row['publication_date']) <= 1980): t1980.extend(row['description'])\n",
    "        elif (1980 < int(row['publication_date']) <= 1990): t1990.extend(row['description'])\n",
    "        elif (1990 < int(row['publication_date']) <= 1995): t1995.extend(row['description'])\n",
    "        elif (1995 < int(row['publication_date']) <= 2000): t2000.extend(row['description'])\n",
    "        elif (2000 < int(row['publication_date']) <= 2002): t2002.extend(row['description'])\n",
    "        elif (2002 < int(row['publication_date']) <= 2005): t2005.extend(row['description'])\n",
    "        elif (2005 < int(row['publication_date']) <= 2007): t2007.extend(row['description'])\n",
    "        elif (2007 < int(row['publication_date']) <= 2010): t2010.extend(row['description'])\n",
    "        elif (2010 < int(row['publication_date']) <= 2012): t2012.extend(row['description'])\n",
    "        elif (2012 < int(row['publication_date']) <= 2015): t2015.extend(row['description'])\n",
    "        elif (2015 < int(row['publication_date']) <= 2020): t2020.extend(row['description'])\n",
    "        \n",
    "    return Counter(t1900), Counter(t1950), Counter(t1960), Counter(t1970), Counter(t1980), \\\n",
    "        Counter(t1990), Counter(t1995), Counter(t2000), Counter(t2002), Counter(t2005), Counter(t2007), \\\n",
    "        Counter(t2010), Counter(t2012), Counter(t2015), Counter(t2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1900,t1950, t1960, t1970, t1980, t1990, t1995, \\\n",
    "t2000, t2002, t2005, t2007, t2010, t2012, t2015,t2020 \\\n",
    "= Counter(), Counter(), Counter(), \\\n",
    "Counter(), Counter(), Counter(), \\\n",
    "Counter(), Counter(), Counter(), Counter(), Counter(), \\\n",
    "Counter() , Counter(), Counter(), Counter()\n",
    "for x in range(1,11):\n",
    "    df = csv_to_df('metadata/metadata%d.csv'%(x))\n",
    "    df = df[df.publication_date != '//']\n",
    "    df[\"publication_date\"] = df[\"publication_date\"].apply(lambda x: x[-4:])\n",
    "    processed = process_all(df)\n",
    "    b, c, d, e, f, g, h, i, j, k, l, m, n, o, p = popular_words_time(processed)\n",
    "    t1900 +=b \n",
    "    t1950 +=c\n",
    "    t1960 += d\n",
    "    t1970 += e\n",
    "    t1980 += f \n",
    "    t1990 += g\n",
    "    t1995 += h\n",
    "    t2000 += i \n",
    "    t2002 += j\n",
    "    t2005 += k \n",
    "    t2007 += l\n",
    "    t2010 += m\n",
    "    t2012 += n\n",
    "    t2015 += o\n",
    "    t2020 += p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see specific names like Kurt Vonnegut appear very commonly in 1960-1970. Also, one thing to notice is the popularity of the word, 'war' starting in 1980 and becoming increasingly popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1800-1900 :   \" + str(t1900.most_common(30)) + \"\\n\")\n",
    "print(\"1900-1950 :   \" + str(t1950.most_common(30)) + \"\\n\")\n",
    "print(\"1950-1960 :   \" + str(t1960.most_common(30)) + \"\\n\")\n",
    "print(\"1960-1970 :   \" + str(t1970.most_common(30)) + \"\\n\")\n",
    "print(\"1970-1980 :   \" + str(t1980.most_common(30)) + \"\\n\")\n",
    "print(\"1980-1990 :   \" + str(t1990.most_common(30)) + \"\\n\")\n",
    "print(\"1990-1995 :   \" + str(t1995.most_common(30)) + \"\\n\")\n",
    "print(\"1995-2000 :   \" + str(t2000.most_common(30)) + \"\\n\")\n",
    "print(\"2000 - 2002 :   \" + str(t2002.most_common(30)) + \"\\n\")\n",
    "print(\"2002 - 2005 :   \" + str(t2005.most_common(30)) + \"\\n\")\n",
    "print(\"2005 - 2007 :   \" + str(t2007.most_common(30)) + \"\\n\")\n",
    "print(\"2007 - 2010 :   \" + str(t2010.most_common(30)) + \"\\n\")\n",
    "print(\"2010 - 2012 :   \" + str(t2012.most_common(30)) + \"\\n\")\n",
    "print(\"2015 - 2020 :   \" + str(t2020.most_common(30)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DONT FORGET: we should look at some words like war and american, history, woman, and visualize the proportion of its use by time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_periods = [t1900, t1950, t1960, t1970, t1980, t1990, t2000, t2002, t2005, t2007, t2010, t2012, t2015, t2020]\n",
    "labels = ['<1900', '<1950', '<1960', '<1970', '<1980', '<1990', '<2000', '<2002', '<2005', '<2007', '<2010', \\\n",
    "          '<2012', '<2015', '<2020']\n",
    "def visualize_word(labels, bins, word):\n",
    "    word_dict = {}\n",
    "    for i in range(len(bins)):\n",
    "        counter = bins[i]\n",
    "        label = labels[i]\n",
    "        word_dict[label] = counter[word] / (sum(counter.values())) * 100\n",
    "\n",
    "    plt.bar(list(word_dict.keys()), word_dict.values(), color='g')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "visualize_word(labels, time_periods, 'war')\n",
    "visualize_word(labels, time_periods, 'love')\n",
    "visualize_word(labels, time_periods, 'man')\n",
    "visualize_word(labels, time_periods, 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words used in REVIEWS, grouped by rating of the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of using the same processing functions, review columns were renamed to \"description\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_csv_to_df(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[pd.notnull(df['text'])]\n",
    "    df = df.rename(index=str, columns={\"text\": \"description\"})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def popular_words_review(processed):\n",
    "    one, two, three, four, five = [], [], [], [], []\n",
    "    for i, row in processed.iterrows():\n",
    "        if (row['rating'] == 1.0): one.extend(row['description'])\n",
    "        elif (row['rating'] == 2.0): two.extend(row['description'])\n",
    "        elif (row['rating'] == 3.0): three.extend(row['description'])\n",
    "        elif (row['rating'] == 4.0): four.extend(row['description'])\n",
    "        elif (row['rating'] == 5.0): five.extend(row['description'])\n",
    "    return Counter(one), Counter(two), Counter(three), Counter(four), Counter(five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one, two, three, four, five = Counter(), Counter(), Counter(), Counter(), Counter()\n",
    "dfs = []\n",
    "for i in range(1,6):\n",
    "    df = review_csv_to_df('reviewdata/reviews%d.csv'%(i))\n",
    "    dfs.append(df)\n",
    "    processed = process_all(df)\n",
    "    a, b, c, d, e = popular_words_review(processed)\n",
    "    one += a\n",
    "    two += b\n",
    "    three += c\n",
    "    four += d\n",
    "    five += e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, words such as 'love', 'really', 'great' and 'like' were used more commonly in books with higher rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rating one :   \" + str(one.most_common(30)) + \"\\n\")\n",
    "print(\"rating two :   \" + str(two.most_common(30)) + \"\\n\")\n",
    "print(\"rating three :   \" + str(three.most_common(30)) + \"\\n\")\n",
    "print(\"rating four :   \" + str(four.most_common(30)) + \"\\n\")\n",
    "print(\"rating five :   \" + str(five.most_common(30)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
